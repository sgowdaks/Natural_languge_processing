{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "4dff55c1-7992-4f69-9187-38152c6a547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "be0eddb4-6158-4bb8-805c-b6d3e316c185",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data= './data/trainDataWithPOS.csv'\n",
    "test_data = './data/testDataWithPOS.csv'\n",
    "df = pd.read_csv(all_data, encoding = \"ISO-8859-1\", names=['sentence#', 'word', 'POS', 'tag'])\n",
    "test_df = pd.read_csv(test_data, encoding = \"ISO-8859-1\", names=['sentence#', 'word', 'POS', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "89a62d92-aec1-40c2-a9fb-cf24775f31e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          sentence#    word    POS  tag\n",
      "0        Sentence #    Word    POS  Tag\n",
      "1       Sentence: 1       I   PRON    O\n",
      "2       Sentence: 1    just    ADV    O\n",
      "3       Sentence: 1     did    AUX    O\n",
      "4       Sentence: 1       a    DET    O\n",
      "...             ...     ...    ...  ...\n",
      "8676  Sentence: 600  breath   NOUN   IF\n",
      "8677  Sentence: 600     the    DET    O\n",
      "8678  Sentence: 600   whole    ADJ    O\n",
      "8679  Sentence: 600    time   NOUN    O\n",
      "8680  Sentence: 600       .  PUNCT    O\n",
      "\n",
      "[8681 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895088f7-4c3f-4d3e-8be8-cdae2654afc9",
   "metadata": {},
   "source": [
    "Loading Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "9ad2f981-5233-49b3-bcd8-f5b8dd01b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = 1\n",
    "end = 600\n",
    "st = []\n",
    "pos = []\n",
    "test = []\n",
    "for i, sentence in enumerate(test_df[\"sentence#\"]):\n",
    "    x = re.findall('[0-9]+', sentence)\n",
    "    if len(x) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        x = int(x[0])\n",
    "        if x != first:\n",
    "            st = \" \".join(st)\n",
    "            test.append((st, pos))\n",
    "            st = []\n",
    "            pos = []\n",
    "            first = first + 1\n",
    "        s = (test_df[['word']].iloc[i]).to_string()\n",
    "        s = list(s)\n",
    "        s = s[8:]\n",
    "        s = \"\".join(s)\n",
    "        st.append(s)\n",
    "        p = test_df[['tag']].iloc[i].to_string()\n",
    "        p = list(p)\n",
    "        p = p[7:]      \n",
    "        p = \"\".join(p)\n",
    "        pos.append(p)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba33b3-2a0e-4e17-a904-2344c9d33569",
   "metadata": {},
   "source": [
    "Loading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "7e2a1bbc-2827-40e4-804c-569b7c5d02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = 1\n",
    "end = 2400\n",
    "st = []\n",
    "pos = []\n",
    "train = []\n",
    "for i, sentence in enumerate(df[\"sentence#\"]):\n",
    "    x = re.findall('[0-9]+', sentence)\n",
    "    if len(x) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        x = int(x[0])\n",
    "        if x != first:\n",
    "            st = \" \".join(st)\n",
    "            train.append((st, pos))\n",
    "            st = []\n",
    "            pos = []\n",
    "            first = first + 1\n",
    "        s = (df[['word']].iloc[i]).to_string()\n",
    "        s = list(s)\n",
    "        s = s[8:]\n",
    "        s = \"\".join(s)\n",
    "        st.append(s)\n",
    "        p = df[['tag']].iloc[i].to_string()\n",
    "        p = list(p)\n",
    "        p = p[7:]      \n",
    "        p = \"\".join(p)\n",
    "        pos.append(p)  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2396ba-3857-45e9-a5db-5e0903d39b45",
   "metadata": {},
   "source": [
    "Splitting into training and the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "d578976f-55bb-4179-8b5f-1e6e6f572520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2399\n",
      "1919\n",
      "480\n",
      "('I walked 1.3 miles at a very brisk pace , and felt loosened up and less sore .', ['O', 'BE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'BF', 'IF', 'O', 'BF', 'IF', 'O'])\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "tr = int(len(train) * 0.8)\n",
    "train_data = train[:tr]\n",
    "test_data = train[tr:]\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "print(train[160])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66327af4-feac-4a2d-b783-7fb935ae7bef",
   "metadata": {},
   "source": [
    "Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "ea846bb0-6bec-4da5-b8d4-dc4362c04865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegClassifier():\n",
    "\n",
    "    def __init__(self, train_corpus, test_corpus):\n",
    "        super().__init__()\n",
    "        print(len(train_corpus))\n",
    "        self.train_words, self.train_labels = self.load_data(train_corpus)\n",
    "        self.test_words, self.test_labels = self.load_data(test_corpus)\n",
    "        self.vectorizer = CountVectorizer(ngram_range=(1, 1)) \n",
    "        self.vectorizer.fit_transform(self.train_words)\n",
    "        self.classifier = LogisticRegression()\n",
    "        self.train()\n",
    "\n",
    "    def load_data(self, corpus):\n",
    "        triplets = []\n",
    "        labels = []\n",
    "        for sample in corpus:\n",
    "            words, tags = sample\n",
    "            words = words.split(\" \")\n",
    "            for index in range(1, len(tags) - 1):\n",
    "                prev_word = words[index - 1]\n",
    "                word = words[index]\n",
    "                next_word = words[index + 1]\n",
    "                triplets.append(' '.join([prev_word, word, next_word]))\n",
    "                labels.append(tags[index])\n",
    "        return triplets, labels\n",
    "\n",
    "    def train(self):\n",
    "        X = self.vectorizer.transform(self.train_words)\n",
    "        y = np.array(self.train_labels)\n",
    "        self.classifier.fit(X, y)\n",
    "\n",
    "    def predict(self, corpus):\n",
    "        X = self.vectorizer.transform(corpus)\n",
    "        return self.classifier.predict(X)\n",
    "\n",
    "    def evaluate(self):      \n",
    "        train_pred = self.predict(self.train_words)\n",
    "        test_pred = self.predict(self.test_words)  \n",
    "        print(\"Evaluating The Test Data\")\n",
    "        print(\"----------------------------------\")\n",
    "        print(classification_report(test_pred, self.test_labels))\n",
    "        \n",
    "    def on_held_out(self, test):\n",
    "        test_w, test_l = self.load_data(test)\n",
    "        y = self.predict(test_w)\n",
    "        print(\"Evaluating The Held Out Test Data\")\n",
    "        print(\"---------------------------------\")\n",
    "        print(classification_report(y, test_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b82658-2e00-408e-8f48-73fbfa7f9070",
   "metadata": {},
   "source": [
    "Evaluating The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "90b81526-8b47-4875-9452-40fa5f7899af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shigo/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating The Test Data\n",
      "----------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          BE       0.42      0.64      0.51       298\n",
      "          BF       0.15      0.53      0.23       142\n",
      "          IE       0.41      0.70      0.51       101\n",
      "          IF       0.27      0.67      0.38        69\n",
      "           O       0.96      0.83      0.89      5168\n",
      "\n",
      "    accuracy                           0.81      5778\n",
      "   macro avg       0.44      0.67      0.51      5778\n",
      "weighted avg       0.90      0.81      0.84      5778\n",
      "\n",
      "Evaluating The Held Out Test Data\n",
      "---------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          BE       0.39      0.62      0.48       373\n",
      "          BF       0.12      0.41      0.19       197\n",
      "          IE       0.32      0.74      0.45       110\n",
      "          IF       0.24      0.72      0.36        74\n",
      "           O       0.96      0.82      0.88      6709\n",
      "\n",
      "    accuracy                           0.80      7463\n",
      "   macro avg       0.41      0.66      0.47      7463\n",
      "weighted avg       0.89      0.80      0.83      7463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogRegClassifier(train_data, test_data)\n",
    "model.evaluate()\n",
    "model.on_held_out(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aa7e51-92ce-46ac-b03d-80bbb5e5e41f",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "c00b229f-a573-4e9f-acc5-67cf49a799bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_words, emb_dim, num_y, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_words, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers=1, bidirectional=False)\n",
    "        self.linear = nn.Linear(hidden_dim, num_y)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embeds = self.emb(text)\n",
    "        out, (last_hidden, last_cell) = self.lstm(embeds.view(len(text), 1, -1))\n",
    "        tag_space = self.linear(out.view(len(text), -1)) \n",
    "        return self.softmax(tag_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22981d7-5870-41f5-892b-6d04578ebf0f",
   "metadata": {},
   "source": [
    "Loading The Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "0fdcd68e-f69b-47d7-9276-ca735274bc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab_tags(train_data):\n",
    "    word_to_ix = {}\n",
    "    tag_to_ix = {} \n",
    "    ix_to_tag = {} \n",
    "    for sent, tags in train_data:\n",
    "        for word in sent.split(\" \"):\n",
    "            if word != \" \":\n",
    "                word_to_ix.setdefault(word, len(word_to_ix))\n",
    "        for tag in tags:\n",
    "            tag_to_ix.setdefault(tag, len(tag_to_ix))\n",
    "            ix_to_tag[tag_to_ix[tag]] = tag\n",
    "    word_to_ix[\"UNK\"] = len(word_to_ix) \n",
    "    return word_to_ix, tag_to_ix, ix_to_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "788a1ece-de25-455b-8b71-540758e77bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'BE': 1, 'BF': 2, 'IE': 3, 'IF': 4}\n"
     ]
    }
   ],
   "source": [
    "tok_to_ix, tag_to_ix, ix_to_tag = load_vocab_tags(train_data)\n",
    "print(tag_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "d16e84e0-7bfe-43c8-826b-fedcac3aca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 50\n",
    "learning_rate = 0.001\n",
    "model = LSTM(len(tok_to_ix), emb_dim, len(tag_to_ix))\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7649bf34-0417-482e-8ea6-e8de8b3b7191",
   "metadata": {},
   "source": [
    "Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "40839d3f-03b2-46a4-abb2-7d0b2b28fda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Training loss: 1.057151198387146\n",
      "\n",
      "Epoch: 1\n",
      "Training loss: 0.8587541580200195\n",
      "\n",
      "Epoch: 2\n",
      "Training loss: 0.7851794958114624\n",
      "\n",
      "Epoch: 3\n",
      "Training loss: 0.7491565346717834\n",
      "\n",
      "Epoch: 4\n",
      "Training loss: 0.7256338596343994\n",
      "\n",
      "Epoch: 5\n",
      "Training loss: 0.7069432735443115\n",
      "\n",
      "Epoch: 6\n",
      "Training loss: 0.690325140953064\n",
      "\n",
      "Epoch: 7\n",
      "Training loss: 0.6745810508728027\n",
      "\n",
      "Epoch: 8\n",
      "Training loss: 0.6590994596481323\n",
      "\n",
      "Epoch: 9\n",
      "Training loss: 0.6435345411300659\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "count = 0\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for text, tags in train_data:\n",
    "        x = [tok_to_ix[tok] for tok in text.split()]\n",
    "        y = [tag_to_ix[tag] for tag in tags]\n",
    "        x_train_tensor = torch.LongTensor(x)\n",
    "        y_train_tensor = torch.LongTensor(y)\n",
    "        pred_y = model(x_train_tensor)\n",
    "        if len(y_train_tensor) != len(pred_y):\n",
    "            count = count + 1\n",
    "        else:\n",
    "            loss = loss_fn(pred_y, y_train_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    print(\"\\nEpoch:\", epoch)\n",
    "    print(\"Training loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "fd41464e-3b5e-4154-83f4-d160dc8e1281",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = []\n",
    "y_test = []\n",
    "y_p = []\n",
    "for sent, tags in test:\n",
    "    x_test.append(sent)\n",
    "    y_test.append(tags)\n",
    "# print(len(x_test))\n",
    "# print(len(y_test))\n",
    "y_tes = []\n",
    "for line in y_test:\n",
    "    for i in line:\n",
    "        y_tes.append(i)\n",
    "# len(y_tes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666eccf8-4d61-4711-9ce9-caa02a4d8889",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "d1532b30-425d-4523-867e-e245fc453539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7740356601794662"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for sentence in x_test:\n",
    "        x = []\n",
    "        for tok in sentence.split():\n",
    "            if tok in tok_to_ix:\n",
    "                x.append(tok_to_ix[tok])\n",
    "            else:\n",
    "                x.append(tok_to_ix[\"UNK\"])      \n",
    "        x_test = torch.LongTensor(x)\n",
    "        pred_y_test = model(x_test)\n",
    "        k = [ix_to_tag[max_ix] for max_ix in pred_y_test.argmax(1).data.numpy()]\n",
    "        for i in k:\n",
    "            y_p.append(i)\n",
    "            \n",
    "# print(len(y_p))\n",
    "y_tes = y_tes[:len(y_p)]\n",
    "print(\"accuracy\")\n",
    "accuracy_score(y_tes, y_p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
